deploy: /home/weiwei/AIMatrix/macro_benchmark/caffe_inference/vgg16_deploy.prototxt
output: prob
batch: 32
device: 1
model: /home/weiwei/AIMatrix/macro_benchmark/caffe_inference/vgg16_iter_500.caffemodel
half2
Input "data": 3x224x224
Output "prob": 1000x1x1
Half2 support requested on hardware without native FP16 support, performance will be negatively affected.
name=data, bindingIndex=0, buffers.size()=2
name=prob, bindingIndex=1, buffers.size()=2
Average over 10 runs is 159.446 ms.
Average over 10 runs is 160.324 ms.
Average over 10 runs is 160.977 ms.
Average over 10 runs is 160.651 ms.
Average over 10 runs is 160.563 ms.
Average over 10 runs is 161.138 ms.
Average over 10 runs is 160.563 ms.
Average over 10 runs is 160.848 ms.
Average over 10 runs is 161.151 ms.
Average over 10 runs is 161.022 ms.
