deploy: /home/weiwei/AIMatrix/macro_benchmark/caffe_inference/vgg16_deploy.prototxt
output: prob
batch: 64
device: 1
model: /home/weiwei/AIMatrix/macro_benchmark/caffe_inference/vgg16_iter_500.caffemodel
half2
Input "data": 3x224x224
Output "prob": 1000x1x1
Half2 support requested on hardware without native FP16 support, performance will be negatively affected.
name=data, bindingIndex=0, buffers.size()=2
name=prob, bindingIndex=1, buffers.size()=2
Average over 10 runs is 324.106 ms.
Average over 10 runs is 323.555 ms.
Average over 10 runs is 323.514 ms.
Average over 10 runs is 324.978 ms.
Average over 10 runs is 324.694 ms.
Average over 10 runs is 325.543 ms.
Average over 10 runs is 324.669 ms.
Average over 10 runs is 325.275 ms.
Average over 10 runs is 327.738 ms.
Average over 10 runs is 326.024 ms.
